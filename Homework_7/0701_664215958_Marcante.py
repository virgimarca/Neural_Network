# -*- coding: utf-8 -*-
"""0701-664215958-Marcante.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q_k1HpF1LD3PV0uweI-gaL5-V5J55a5c
"""

!nvidia-smi -L

import os
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn import LSTM

with open('names.txt') as f:
    data = f.readlines()

text = []
for line in data:
  line = line.replace('\n','&')
  line = line.lower()
  text.append(line)

alphabet = '&abcdefghijklmnopqrstuvwxyz'
char_to_int = dict((c, i) for i, c in enumerate(alphabet))
int_to_char = dict((i, c) for i, c in enumerate(alphabet))

X = []
Y = []
for name in text:
  x = name.ljust(11, '&')[:11]
  y = x[1:] + '&'
        
  x_onehot = torch.zeros((11, 27))
  y_onehot = torch.zeros(11)
  for i, char in enumerate(x):
      x_onehot[i, char_to_int[char]] = 1
  for i, char in enumerate(y):
      y_onehot[i] = char_to_int[char]
  
  X.append(x_onehot)
  Y.append(y_onehot)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
input_size = 27
hidden_size = 64
output_size = 27
num_layers = 1

class LSTM(nn.Module):
    
    def __init__(self, input_size, hidden_size, output_size, num_layers):
        super(LSTM, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.fc3 = nn.Linear(output_size, output_size)
        
    def forward(self, X, states):
        h, c = states
        out, (h, c) = self.lstm1(X, (h, c))
        out = F.relu(self.fc2(out))
        out = self.fc3(out)
        return out, (h, c)
        
    def sample_names(self, start='a', n=10, k=5):
        generated_names = []
        while len(generated_names)+1 <= n:
            with torch.no_grad():
                h = torch.zeros((num_layers, 1, hidden_size)).to(device)
                c = torch.zeros((num_layers, 1, hidden_size)).to(device)
                name = start
                
                for char in start:
                    x = torch.zeros((1, 1, 27))
                    x[0, 0, char_to_int[char]] = 1
                    x = x.to(device)
                    out, (h, c) = self(x, (h, c))
                _, ids = torch.topk(out[0], k)
                id = np.random.choice(ids.cpu().numpy()[0])
                letter = int_to_char[id]
                name += letter

                while letter != '&':
                    x = torch.zeros((1, 1, 27))
                    x[0, 0, char_to_int[letter]] = 1
                    x = x.to(device)
                    out, (h, c) = self(x, (h, c))
                    _, ids = torch.topk(out[0], k)
                    id = np.random.choice(ids.cpu().numpy()[0])
                    letter = int_to_char[id]
                    name += letter
            
                if name[-1] != '&':
                    name += '&'

            generated_names.append(name)
        generated_names = [name[:-1].title() for name in generated_names]
        return generated_names

model = LSTM(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers).to(device)

lr = 1e-2
gamma = 0.9

criterion = nn.CrossEntropyLoss(reduction='mean')
optimizer = optim.Adam(model.parameters(), lr=lr)
lr_scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=len(X), gamma=gamma)
epochs = 50

epoch_losses = []
for epoch in range(1, epochs+1):
    epoch_loss = []
    
    for (x, y) in zip(X, Y):
        x, y = x.to(device), y.to(device)
        
        h = torch.zeros((num_layers, hidden_size)).to(device)
        c = torch.zeros((num_layers, hidden_size)).to(device)

        optimizer.zero_grad()
        y_pred, (h, c) = model(x, (h, c))
        y_pred = y_pred.transpose(0, 1)

        loss = criterion(y_pred.view(1, 27, 11), y.view(1, 11).long())
        loss.backward(retain_graph=True)
        optimizer.step()
        lr_scheduler.step()
        epoch_loss.append(loss.item())
        
    mean_epoch_loss = np.mean(epoch_loss)
    epoch_losses.append(mean_epoch_loss)
    
    if epoch % 5 == 0:    
        print(f'Epoch:{epoch}    Loss:{mean_epoch_loss}')

path = './0702-664215958-Marcante.ZZZ'
torch.save(model.state_dict(), path)

plt.figure(figsize=(4, 3))
ax = plt.axes()
ax.plot(epoch_losses)
ax.set_title("Loss")
plt.xlabel("Epochs")
plt.savefig('./drive/MyDrive/loss.eps', format = 'eps')
plt.show()