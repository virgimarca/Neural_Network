# -*- coding: utf-8 -*-
"""0703-664215958-Marcante.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EPfZOhjGbVQOpR1lR9FgPnuVa5K97Mji
"""

!nvidia-smi -L

import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import LSTM

alphabet = '&abcdefghijklmnopqrstuvwxyz'
char_to_int = dict((c, i) for i, c in enumerate(alphabet))
int_to_char = dict((i, c) for i, c in enumerate(alphabet))
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

class LSTM(nn.Module):
    
    def __init__(self, input_size, hidden_size, output_size, num_layers):
        super(LSTM, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.fc3 = nn.Linear(output_size, output_size)
        
    def forward(self, X, states):
        h, c = states
        out, (h, c) = self.lstm1(X, (h, c))
        out = F.relu(self.fc2(out))
        out = self.fc3(out)
        return out, (h, c)
        
    def sample_names(self, start='a', n=10, k=5):
        generated_names = []
        while len(generated_names)+1 <= n:
            with torch.no_grad():
                h = torch.zeros((num_layers, 1, hidden_size)).to(device)
                c = torch.zeros((num_layers, 1, hidden_size)).to(device)
                name = start
                
                for char in start:
                    x = torch.zeros((1, 1, 27))
                    x[0, 0, char_to_int[char]] = 1
                    x = x.to(device)
                    out, (h, c) = self(x, (h, c))
                _, ids = torch.topk(out[0], k)
                id = np.random.choice(ids.cpu().numpy()[0])
                letter = int_to_char[id]
                name += letter

                while letter != '&':
                    x = torch.zeros((1, 1, 27))
                    x[0, 0, char_to_int[letter]] = 1
                    x = x.to(device)
                    out, (h, c) = self(x, (h, c))
                    _, ids = torch.topk(out[0], k)
                    id = np.random.choice(ids.cpu().numpy()[0])
                    letter = int_to_char[id]
                    name += letter
            
                if name[-1] != '&':
                    name += '&'

            generated_names.append(name)
        generated_names = [name[:-1].title() for name in generated_names]
        return generated_names

input_size = 27
hidden_size = 64
output_size = 27
num_layers = 1
path = './0702-664215958-Marcante.ZZZ'
model = LSTM(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)
model.load_state_dict(torch.load(path))
model = model.to(device)

model.sample_names(start='a', n=20, k=4)

