# -*- coding: utf-8 -*-
"""0601-664215958-Marcante.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zYhIctGg929Hu2RcWgIvqgXFOv2t-s6a
"""

!nvidia-smi -L

import matplotlib.pyplot as plt
import numpy as np
import torch
from torchvision import io
from PIL import Image
import os
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR
from torchvision.datasets import ImageFolder

!unzip geometry_dataset.zip

data_dir = '/content/output'
os.mkdir(os.path.join('/content', 'train'))
os.mkdir(os.path.join('/content', 'test'))

filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png']
counter_img = {'Heptagon': 0, 'Nonagon': 0, 'Triangle': 0, 'Octagon': 0, 'Star': 0, 'Pentagon': 0, 'Square': 0, 'Circle': 0, 'Hexagon': 0}
for k in counter_img:
  os.mkdir(os.path.join('/content/train/', k))
  os.mkdir(os.path.join('/content/test/', k))

for filename in filenames:
  img = Image.open('/content/output/' + filename)
  for k, v in counter_img.items():
      if filename.startswith(k):
        if v < 8000:
          img.save('/content/train/' + k + '/' + filename, )
          counter_img[k] += 1
        else:
          img.save('/content/test/' + k + '/' + filename, )

#filenames = [name for name in os.listdir('/content/train/Star') if name.startswith('Star')]
#len(filenames)

is_cuda = False
if torch.cuda.is_available():
    is_cuda = True

simple_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
        ])
train = ImageFolder('/content/train/', simple_transform)
test = ImageFolder('/content/test/', simple_transform)
train_data_loader = torch.utils.data.DataLoader(train,batch_size=100,shuffle=True)
test_data_loader = torch.utils.data.DataLoader(test,batch_size=100,shuffle=True)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, 5)
        self.pool1 = nn.MaxPool2d(4, 4)
        self.conv2 = nn.Conv2d(64, 32, 5)
        self.pool2 = nn.MaxPool2d(5, 5)
        self.conv3 = nn.Conv2d(32, 16, 5)
        self.pool3 = nn.MaxPool2d(5, 5)
        self.fc1 = nn.Linear(16, 200)
        self.fc2 = nn.Linear(200, 64)
        self.fc3 = nn.Linear(64, 9)
        
    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = self.pool2(x)
        x = self.conv3(x)
        x = F.relu(x)
        x = self.pool3(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        return x

def train(batch_size, model, device, train_loader, optimizer, epoch):
    model.train()
    tot_loss = 0
    correct = 0
    for batch_idx, (x, y) in enumerate(train_loader):
        x, y = x.to(device), y.to(device)

        optimizer.zero_grad()
        y_prob = model(x)
        loss = torch.nn.CrossEntropyLoss()(y_prob, y)
        loss.backward()
        optimizer.step()

        pred = y_prob.argmax(dim=1, keepdim=True) 
        correct += pred.eq(y.view_as(pred)).sum().item()

        tot_loss = tot_loss + loss.item()
        
        tr_loss = tot_loss/(len(train_loader))
        acc = 100.0*correct/(len(train_loader)*batch_size)
          
    print(f'End of Epoch: {epoch}')
    print(f'Training Loss: {tr_loss:.6f}, Training Accuracy: {acc:.2f}%'.format())
    return  tr_loss, acc

def test(test_batch_size, model, device, test_loader):
    model.eval()
    tot_loss = 0
    correct = 0

    with torch.no_grad():
        for x, y in test_loader:

            x, y = x.to(device), y.to(device)

            y_prob = model(x)

            tot_loss += torch.nn.CrossEntropyLoss()(y_prob, y).item()  
            pred = y_prob.argmax(dim=1, keepdim=True)
            correct += pred.eq(y.view_as(pred)).sum().item()

            te_loss = tot_loss/(len(test_loader))
            acc = 100.0*correct/(len(test_loader)*test_batch_size)

    print(f'Test Loss: {te_loss:.6f}, Test Accuracy: {acc:.2f}%'.format())
    return te_loss, acc

# Training settings

batch_size=100
n_epochs=20
lr=1e-3
gamma=0.7
torch.manual_seed(2022)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

train_losses = []
test_losses = []
train_accuraces = []
test_accuraces = []

model = Net().to(device)
optimizer = optim.Adam(model.parameters(), lr=lr)

scheduler = StepLR(optimizer, step_size=1, gamma=gamma)

for epoch in range(n_epochs):
    loss_tr, acc_tr = train(batch_size, model, device, train_data_loader, optimizer, epoch)
    train_losses.append(loss_tr)
    train_accuraces.append(acc_tr)

    loss_te, acc_te = test(batch_size,model, device, test_data_loader)
    test_losses.append(loss_te)
    test_accuraces.append(acc_te)

    scheduler.step()

torch.save(model.state_dict(), "/content/drive/MyDrive/Neural_Networks/0602-664215958-Marcante.pt")

plt.figure(figsize=(4, 3))
ax = plt.axes()
ax.plot(range(1, len(train_losses)+1), train_losses, 'b-', label='training loss')
ax.plot(range(1, len(test_losses)+1), test_losses, 'r-', label='test loss')
ax.set_xlabel('number of epochs')
ax.set_ylabel('loss')
lgd = ax.legend(loc = 'upper center', bbox_to_anchor = (0.5, -0.2), ncol = 2, fancybox = True)
plt.savefig('/content/drive/MyDrive/Neural_Networks/loss.eps', format='eps', bbox_extra_artists = (lgd,), bbox_inches = 'tight') 
plt.show()

plt.figure(figsize=(4, 3))
ax = plt.axes()
ax.plot(range(1, len(train_accuraces)+1), train_accuraces, 'b-', label='training loss')
ax.plot(range(1, len(test_accuraces)+1), test_accuraces, 'r-', label='test loss')
ax.set_xlabel('number of epochs')
ax.set_ylabel('accuracy')
lgd = ax.legend(loc = 'upper center', bbox_to_anchor = (0.5, -0.2), ncol = 2, fancybox = True)
plt.savefig('/content/drive/MyDrive/Neural_Networks/accuracy.eps', format='eps', bbox_extra_artists = (lgd,), bbox_inches = 'tight') 
plt.show()